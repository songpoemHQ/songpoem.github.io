---
title:  Implement Guardrails to enhance safety of your AI 
date:   2025-01-20 21:13:30 +0530
author_profile: true
author: Sandesh Soni
category: Artificial Intelligence
tags:
- safety
- guardrails
- artificial-intelligence
read_time: false
comments: true
share: true
related: true
breadcrumbs: true
published: true
folder-url: assets/images/blog-safety-mechanisms-in-ai
---

### Guardrails ensures LLMs and AI operate within the defined boundary.
Within ethical, legal, and technical boundaries as defined.

Guardrails in AI are like those walls, but they're invisible! They're special rules and instructions that keep the AI safe and helpful.


Just like how walls stop you from falling, guardrails stop AI from:
- Saying mean or untrue things
- Giving out private information
- Doing things that could be harmful
We will see details below.

![Guard Rails]({{site.baseurl}}/{{page.folder-url}}/guardrails.png)

A few frameworks available for implementing guardrails
- [Nemo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails){:target="_blank"}
- [Guardrails AI](https://github.com/guardrails-ai/guardrails){:target="_blank"}
- [PurpleLLama / Llama-Guard3](https://github.com/meta-llama/PurpleLlama){:target="_blank"}



